{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Develop Scoring Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will develop the scoring script and test it locally. We will use the scoring script to create the web service that will call the model for scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "from utilities import text_to_json, get_auth\n",
    "import logging\n",
    "from dotenv import set_key, get_key, find_dotenv\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.core.model import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('./scripts/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_path = find_dotenv(raise_error_if_not_found=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config(auth=get_auth(env_path))\n",
    "print(ws.name, ws.resource_group, ws.location, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's retrive the model registered earlier and download it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'question_match_model'\n",
    "model_version = int(get_key(env_path, 'model_version'))\n",
    "model = Model(ws, name=model_name, version=model_version)\n",
    "print(model.name, model.version, model.url, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.download(target_dir=\".\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Scoring Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the writefile magic to write the contents of the below cell to `score.py` which includes the  `init` and `run` functions required by AML.\n",
    "- The init() function typically loads the model into a global object.\n",
    "- The run(input_data) function uses the model to predict a value based on the input_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile score.py\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "from duplicate_model import DuplicateModel\n",
    "import logging\n",
    "import timeit as t\n",
    "\n",
    "def init():\n",
    "    logger = logging.getLogger(\"scoring_script\")\n",
    "    global model\n",
    "    model_path = \"model.pkl\"\n",
    "    questions_path = \"./data_folder/questions.tsv\"\n",
    "    start = t.default_timer()\n",
    "    model = DuplicateModel(model_path, questions_path)\n",
    "    end = t.default_timer()\n",
    "    loadTimeMsg = \"Model loading time: {0} ms\".format(round((end-start)*1000, 2))\n",
    "    logger.info(loadTimeMsg)\n",
    "\n",
    "\n",
    "def run(body):\n",
    "    logger = logging.getLogger(\"scoring_script\")\n",
    "    json_load_text = json.loads(body)\n",
    "    text_to_score = json_load_text[\"input\"]\n",
    "    start = t.default_timer()\n",
    "    resp = model.score(text_to_score)\n",
    "    end = t.default_timer()\n",
    "    logger.info(\"Prediction took {0} ms\".format(round((end-start)*1000, 2)))\n",
    "    return json.dumps(resp)\n"
    "\n",
    "@app.route(\"/isready\", methods=[\"GET\"])\n",
    "def is_ready(body):\n",
    "    return \"1\"\n",
    "\n",
    "@app.route(\"/metadata\", methods=[\"GET\"])\n",
    "def is_ready(body):\n",
    "    return {\n",
    "        \"azEnvironment\": \"Azure\",\n",
    "        \"location\": \"westus2\",\n",
    "        \"osType\": \"Ubuntu 16.04\",\n",
    "        \"resourceGroupName\": \"\",\n",
    "        \"resourceId\": \"\",\n",
    "        \"sku\": \"\",\n",
    "        \"subscriptionId\": \"\",\n",
    "        \"uniqueId\": \"PythonMLRST\",\n",
    "        \"vmSize\": \"\",\n",
    "        \"zone\": \"\",\n",
    "        \"isServer\": False,\n",
    "        \"version\": \"\"\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test by running the score.py which will bring the imports and functions into the context of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run score.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use one of the duplicate questions to test our driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dupes_test_path = './data_folder/dupes_test.tsv'\n",
    "dupes_test = pd.read_csv(dupes_test_path, sep='\\t', encoding='latin1')\n",
    "text_to_score = dupes_test.iloc[0,4]\n",
    "text_to_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, call the init() to initalize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert the question text to json format and make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsontext = text_to_json(text_to_score)\n",
    "r = run(jsontext)\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we move on to [creating the docker image which we will deploy](04_CreateImage.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:MLAKSDeployAML]",
   "language": "python",
   "name": "conda-env-MLAKSDeployAML-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
